# Copyright (C) 2014 Embecosm Limited.

# Contributor Andrew Burgess <andrew.burgess@embecosm.com>

# This file is part of Embench and was formerly part of the Bristol/Embecosm
# Embedded Benchmark Suite.

# SPDX-License-Identifier: GPL-3.0-or-later

# Called for each binary within each benchmark.  Return true
# (non-zero) if we should run this benchmark, otherwise, return false
# (zero).
proc beebs_filter_benchmark { benchmark filename } {
    return 1
}

# Called once before we run any benchmark tests.
proc beebs_setup_before_all_benchmarks { } {
    return 1;
}

# Called once after we've finished all the benchmark tests.
proc beebs_cleanup_after_all_benchmarks { } {
    return 1
}

# Called once for every benchmark directory before any tests in that
# directory are run.
proc beebs_setup_before_benchmark { benchmark filename } {
    return 1;
}

# Called once for every benchmark directory after all the tests in
# that directory are run.
proc beebs_cleanup_after_benchmark { benchmark filename } {
    return 1
}

# Give an announcement before running a benchmark.
proc beebs_announce { benchmark filename } {
    verbose -log "\n - - - - -     $benchmark     - - - - -\n"
}

# Method called to run benchmarks.  Passed the path to the benchmark
# directory DIR, the benchmark name BENCHMARK, and the binary to
# execute FILE.  The parameter FILE will not include the directory
# suffix, while the path DIR does already include the benchmark name.
proc beebs_run_benchmark { benchmark filename } {
    global beebs_run_benchmark_result
    set beebs_run_benchmark_result [beebs_load "${filename}"]
    return 1
}

# Main loop to run all benchmarks.
proc beebs_run_all_benchmarks { board objdir } {
    global runtests
    global beebs_run_benchmark_result

    # Perform any setup required before running any benchmarks.
    if { [catch {beebs_setup_before_all_benchmarks} msg] } {
        error "Failed to setup before all benchmarks\n$msg"
    }

    if {[board_info ${board} exists beebs,benchmarks]} {
       set benchmarks [board_info ${board} beebs,benchmarks]
    } else {
       return 0
    }

    # First, need to find all of the benchmark files.  The BENCHMARKS
    # variable contains all the directories that can contain
    # benchmarks, the benchmarks themselves are the executable files
    # within those directories.

    # This can be overridden from the command line (for example by "make check
    # RUNTESTFLAGS=execute.exp=mergesort"), in which case the args to
    # RUNTESTFLAGS will appear in the gloval variable "runtests", with the
    # first argument being the ".exp" file, which we must discard.

    set tests_to_run [lindex $runtests 1]
    if { [llength $tests_to_run] > 0 } {
	set benchmarks [split $tests_to_run ","]
    }

    foreach benchmark $benchmarks {

        # Now find all the executable files to run.
        set benchmark_dir "$objdir/$benchmark"
        set benchmark_file "${benchmark_dir}/${benchmark}"

        # Now find the executable and check it is valid. It is OK not to find
        # the file - that just implies it was not one of the ones built.

        if { [file isfile ${benchmark_file}] && [file executable ${benchmark_file}]} {

            # Filter.
            if { ![beebs_filter_benchmark $benchmark $benchmark_file] } {
                unsupported "$benchmark"
                continue;
            }

            beebs_announce $benchmark $benchmark_file

            # Setup.
            if { [catch {beebs_setup_before_benchmark $benchmark $benchmark_file} msg] } {
                verbose -log "Failed to setup before benchmark '$benchmark' ($benchmark_file)\n$msg"
                unresolved "$benchmark"
            }

            # Run.
            if { [catch {beebs_run_benchmark $benchmark $benchmark_file} msg] } {
                fail "$benchmark"
                error "Failed to run benchmark '$benchmark' ($benchmark_file)\n$msg"
            } {
                if { [lindex $beebs_run_benchmark_result 0 ] == "pass" } {
                    pass "$benchmark: execute"
                } {
                    verbose -log "Fail $benchmark: $beebs_run_benchmark_result"
                    fail "$benchmark: execute"
                }
                # if beebs_run_benchmark returned an execution time as well as
                # pass/fail, the value is saved for printing at the end of this
                # proc
                set exec_time [lindex $beebs_run_benchmark_result 1 ]
                if { $exec_time != "" } {
                    set results($benchmark) $exec_time
                    pass "$benchmark: measure execution time"
                } {
                    verbose -log "Fail $benchmark: no execution time returned by beebs_run_benchmark"
                    fail "$benchmark: measure execution time"
                }
            }

            # Cleanup.
            if { [catch {beebs_cleanup_after_benchmark $benchmark $benchmark_file} msg] } {
                error "Failed to cleanup after benchmark '$benchmark' ($benchmark_file)\n$msg"
            }
        }
    }

    send_user "\n"
    send_user "\t\t=== Execution time table ===\n"
    send_user "\n"
    send_user    [format "%-24s  %9s\n" "Benchmark" "Time"]
    send_user -- [format "%-24s  %9s\n" "---------" "----"]

    set exec_time_tot [expr {0}]

    foreach r [lsort [array names results]] {
       set l $results($r)
       set exec_time_val [lindex $l 0]

       set exec_time_tot [expr {$exec_time_tot + $exec_time_val}]

       send_user [format "%-24s  %9d\n" $r $exec_time_val]
    }

    send_user -- [format "%-24s  %9s\n" "---------" "----"]
    send_user [format "%-24s  %9d\n" "Total" $exec_time_tot]

    # Now cleanup after we've finished running all the benchmarks.
    if { [catch {beebs_cleanup_after_all_benchmarks} msg] } {
        error "Failed to cleanup after all benchmarks\n$msg"
    }
}

proc beebs_results_root { } {
    set dir [pwd]
    set dir "$dir/results"
    file mkdir ${dir}
    return ${dir}
}

proc beebs_results_directory { benchmark file } {
    set dir [beebs_results_root]
    set dir "${dir}/${benchmark}/${file}/"
    file mkdir ${dir}
    return ${dir}
}

proc beebs_init { test_file_name } {
    # ... nothing yet ...
}

proc beebs_finish {} {
    # ... nothing yet ...
}


# Main loop to size all benchmarks.

# Return 1 if successful, 0 otherwise.

proc beebs_size_all_benchmarks { board objdir } {

    # First, need to find all of the benchmark files.  The BENCHMARKS
    # variable contains all the directories that can contain
    # benchmarks, the benchmarks themselves are the executable files
    # within those directories.

    if {[board_info ${board} exists beebs,benchmarks]} {
	set benchmarks [board_info ${board} beebs,benchmarks]
    } else {
	return 0
    }

    # The default 'size' program as shipped in the latest GNU binutils
    # release (as of Feb 2019) counts read only data as text.  A
    # change has been proposed that would count read only data as
    # data, but this has not yet been merged.
    #
    # The following block probes size to see if it supports the new
    # format, and if it does it makes use of it, otherwise it uses the
    # old version of size.
    #
    # The obvious problem here is that people using the same toolchain
    # and beebs could get different results if they have a different
    # version of size installed.  That's a risk, but for now, allowing
    # accurate results seems worth it.

    global env
    if { [llength [array names env SIZE_PROG]] > 0 } {
	set size_prog $env(SIZE_PROG)
    } else {
	set size_prog "size"
    }

    verbose -log "size_prog set to $size_prog"

    set use_size_gnu_format 0
    set size_args ""
    verbose -log "Check if 'size --format=GNU' is supported"
    spawn ${size_prog} --format=GNU
    expect {
        -re {: invalid argument to --format:} {
            verbose -log "GNU format not supported"
            set use_size_gnu_format 0
            set size_args "--format=Berkeley --common"
            warning "The version of 'size' found counts read only data as text"
        }
        -re {: 'a.out': No such file} {
            verbose -log "GNU format is supported"
            set use_size_gnu_format 1
            set size_args "--format=GNU --common"
        }
    }

    foreach benchmark [regexp -all -inline {\S+} ${benchmarks}] {

        # Now find the executable and check it is valid. It is OK not to find
        # the file - that just implies it was not one of the ones built.

        set benchmark_file "${objdir}/${benchmark}/${benchmark}"

        if { [file isfile ${benchmark_file}] &&
	     [file executable ${benchmark_file}]} {
	    set timeout 1
	    spawn ${size_prog} ${size_args} ${benchmark_file}

	    expect {
		-re {\s*(\d+)\s+(\d+)\s+(\d+)\s+(\d+\s+[[:xdigit:]]+\s+)?[[:alnum:]_/.-]+} {
		    set text_val $expect_out(1,string)
		    set data_val $expect_out(2,string)
		    set bss_val  $expect_out(3,string)
		    set results($benchmark) [list $text_val $data_val $bss_val]
		    pass "$benchmark\n"
		}

		eof {
		    fail "$benchmark: EOF\n"
		}

		timeout {
		    unresolved "$benchmark: timeout\n"
		}
	    }
	}
    }

    send_user "\n"
    send_user "\t\t=== Size table ===\n"
    send_user "\n"
    send_user [format "%-20s  %6s %6s %6s\n" "Benchmark" "Text" "Data" "BSS"]
    send_user -- [format "%-20s  %6s %6s %6s\n" "---------" "----" "----" "---"]

    set text_tot [expr {0}]
    set data_tot [expr {0}]
    set bss_tot  [expr {0}]

    foreach r [lsort [array names results]] {
	set l $results($r)
	set text_val [lindex $l 0]
	set data_val [lindex $l 1]
	set bss_val  [lindex $l 2]

	set text_tot [expr {$text_tot + $text_val}]
	set data_tot [expr {$data_tot + $data_val}]
	set bss_tot  [expr {$bss_tot  + $bss_val}]

	send_user [format "%-20s  %6d %6d %6d\n" $r $text_val $data_val $bss_val]
    }

    send_user -- [format "%-20s  %6s %6s %6s\n" "---------" "----" "----" "---"]
    send_user [format "%-20s  %6d %6d %6d\n" "Total" $text_tot $data_tot $bss_tot]
}

